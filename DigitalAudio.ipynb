{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Cupig4UqfQQsq4JmybIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijjus/ST-summer-2024/blob/main/DigitalAudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference between Analog & Digital\n",
        "\n",
        "Imagine you have a record player and a CD player. Both play music, but they work in fundamentally different ways. That difference is all about analog vs. digital.\n",
        "\n",
        "**Analog:**\n",
        "\n",
        "* Think of an old record player with a vinyl record. The grooves on the record represent the ups and downs of sound waves. A needle reads these grooves, and those movements are converted into electrical signals that your speakers can turn back into sound.\n",
        "* It's like a physical picture of the sound wave. Imagine drawing a squiggly line on a piece of paper to represent how the sound wave moves back and forth. An analog signal is similar, but with physical variations representing the sound.\n",
        "* Analog signals are continuous, meaning they change smoothly over time, just like the grooves on a record.\n",
        "\n",
        "**Digital:**\n",
        "\n",
        "* Now think of a CD player. CDs store music as a series of ones and zeros, like a computer code. This code represents the sound wave by breaking it down into tiny pieces, kind of like taking a bunch of snapshots of that squiggly line on paper.\n",
        "* It's like a bunch of dots that together create the picture of the sound wave. The more dots you have, the clearer the picture.\n",
        "* Digital signals are discrete, meaning they only have specific values (ones and zeros) at specific points in time.\n"
      ],
      "metadata": {
        "id": "U0-IPktxtfzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analog to Digital Conversion of Audio\n",
        "\n",
        "Converting analog audio (like speech) to a digital format like MP3 involves several steps:\n",
        "\n",
        "**1. Analog to Digital Conversion (ADC):**\n",
        "\n",
        "* The first step is to convert the continuous analog audio signal (variations in voltage representing sound waves) into a digital format that computers can understand.\n",
        "* This is done using an **analog-to-digital converter (ADC)**.\n",
        "* The ADC samples the analog signal at a specific rate (measured in samples per second or Hz). The higher the sampling rate, the more accurately the digital representation captures the original sound.\n",
        "* During sampling, the ADC assigns a digital value (usually a binary number) to the amplitude (strength) of the analog signal at each sample point.\n",
        "\n",
        "**2. Quantization:**\n",
        "\n",
        "* The continuous range of analog signal amplitudes is converted into discrete values during sampling. This process is called **quantization**.\n",
        "* The number of bits used to represent each sample determines the quantization level. More bits allow for a wider range of values and a more accurate representation of the original signal. However, this also increases the file size.\n",
        "\n",
        "**3. Encoding:**\n",
        "\n",
        "* Once the audio is in digital format (a series of numbers representing sampled amplitudes), it needs to be encoded for storage or transmission.\n",
        "* Audio codecs like MP3 come into play here. These codecs use algorithms to compress the digital audio data while preserving as much quality as possible.\n",
        "* MP3 uses a technique called **psychoacoustic modeling** to remove sounds that are less audible to the human ear. This allows for significant file size reduction compared to uncompressed audio formats like WAV or PCM.\n",
        "\n",
        "**4. Additional Processing (Optional):**\n",
        "\n",
        "* Depending on the desired outcome, additional processing steps like applying filters or adjusting bitrate (data transfer rate) might be used.\n",
        "\n",
        "**5. Saving the File:**\n",
        "\n",
        "* The final compressed audio data is then saved in a specific file format, such as MP3 (.mp3), which allows for playback on compatible devices and software.\n",
        "\n",
        "**Here's a quick summary:**\n",
        "\n",
        "1. Analog audio ->\n",
        "2. Sampling (ADC) -> Digital representation (series of numbers) ->\n",
        "3. Quantization (assigning discrete values) ->\n",
        "4. Encoding (compression with MP3 or other codec) ->\n",
        "5. Saving as a digital file (.mp3)\n",
        "\n",
        "By following these steps, analog audio like speech is transformed into a digital format suitable for storage, transmission, and playback on various digital devices. The specific details of the process, such as sampling rate, bitrate, and chosen codec, can impact the resulting audio quality and file size.\n"
      ],
      "metadata": {
        "id": "fK8Lj0wus36V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch the following video:\n",
        "\n",
        "https://www.youtube.com/watch?v=spUNpyF58BY&pp=ygUTM2JsdWUxYnJvd24gZm91cmllcg%3D%3D"
      ],
      "metadata": {
        "id": "4I4Lrn8hsTNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python package for working with audio files\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mn8SmNQgsYFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig, rate = librosa.load(\"/content/sample-12s.mp3\", sr=44100, mono=True)"
      ],
      "metadata": {
        "id": "RUC_R20du1Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sig)"
      ],
      "metadata": {
        "id": "sRm19cHavofb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig.size"
      ],
      "metadata": {
        "id": "UgKXmHnFvzxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig.nbytes"
      ],
      "metadata": {
        "id": "AuCY_ysDv2Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the Fourier transform\n",
        "fft = np.fft.fft(sig)"
      ],
      "metadata": {
        "id": "7bNR2xn3v6uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magnitude = np.abs(fft)\n",
        "magnitude"
      ],
      "metadata": {
        "id": "WQgcdmv1wxBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency = np.linspace(0, rate, len(magnitude))\n",
        "frequency"
      ],
      "metadata": {
        "id": "9ARljT_QwzKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(frequency, magnitude, 'r')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xcVexDi3w3qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        "\n",
        "What are some of the file types in which audio data is stored? We've seen mp3, which is very commmon. Find out 2-3 more."
      ],
      "metadata": {
        "id": "AxatS0b1xEq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will install a package to help with conversion of files\n",
        "!pip3 install pydub"
      ],
      "metadata": {
        "id": "oWqvnlf2xfT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "bLF77w9Dw8GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sound = AudioSegment.from_mp3(\"/content/sample-12s.mp3\")\n",
        "sound.export(\"/content/sample-12s.wav\", format=\"wav\")"
      ],
      "metadata": {
        "id": "QaaCNLcSxeGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/sample-12s.wav"
      ],
      "metadata": {
        "id": "L4y3OfqAyAJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/sample-12s.mp3"
      ],
      "metadata": {
        "id": "dD_o110CyMUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        "\n",
        "What do you notice about the size of the files? Why is that? Research about the mp3 and wav format, and how they are different."
      ],
      "metadata": {
        "id": "xnddqhviyfQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mel Spectrogram\n",
        "\n",
        "Next up, we will look into something called the Mel Spectrogram.\n",
        "\n",
        "A mel spectrogram is a visual representation of an audio signal that takes into account how humans perceive sound. It's a powerful tool used in various audio analysis applications, like speech recognition, music information retrieval, and speaker identification.\n",
        "\n",
        "Here's a breakdown of the key aspects of a mel spectrogram:\n",
        "\n",
        "**1. Spectrogram Basics:**\n",
        "\n",
        "* A regular spectrogram displays the frequency content of an audio signal over time. It essentially breaks down the sound into its component frequencies and shows how the intensity (strength) of each frequency changes over time. This creates a visual representation where the x-axis represents time, the y-axis represents frequency (in Hertz, Hz), and color intensity indicates the strength of that frequency at a specific time.\n",
        "\n",
        "**2. Mel Scale:**\n",
        "\n",
        "* Human ears don't perceive sound equally across the entire frequency spectrum. We are more sensitive to changes in lower frequencies compared to higher frequencies. The mel scale is a perceptual scale that approximates this non-linear human perception of pitch. It essentially compresses the high-frequency range and expands the low-frequency range on a linear scale.\n",
        "\n",
        "**3. Mel Filter Banks:**\n",
        "\n",
        "* To create a mel spectrogram, the audio signal is passed through a series of mel filter banks. These filters mimic the human auditory system's frequency response. Each filter captures a specific range of frequencies on the mel scale.\n",
        "\n",
        "**4. Mel Spectrogram Visualization:**\n",
        "\n",
        "* The output of the mel filter banks is then displayed as a mel spectrogram. The x-axis still represents time, but the y-axis now represents frequency on the mel scale. The color intensity at each point in the spectrogram indicates the strength of the signal within that specific frequency band based on the mel scale.\n",
        "\n",
        "**Benefits of Mel Spectrograms:**\n",
        "\n",
        "* **Human-centric representation:** Mel spectrograms provide a more intuitive way to analyze audio information as they consider human auditory perception.\n",
        "* **Focus on relevant frequencies:** By focusing on frequencies that humans perceive better, mel spectrograms can be more effective for tasks like speech recognition.\n",
        "* **Reduced dimensionality:** Mel spectrograms reduce the complexity of the original audio signal by focusing on perceptually relevant information, making it easier for machine learning algorithms to process audio data.\n",
        "\n",
        "**Applications of Mel Spectrograms:**\n",
        "\n",
        "* **Speech Recognition:** Mel spectrograms are widely used in speech recognition systems to help machines understand spoken language. Analyzing the time-frequency patterns in the spectrogram allows the system to identify phonemes (basic units of speech) and ultimately recognize words and sentences.\n",
        "* **Music Information Retrieval:** Mel spectrograms can be used to identify songs, classify music genres, and analyze musical content. By comparing the mel spectrograms of different audio recordings, similarities and differences in their frequency content can be identified.\n",
        "* **Speaker Identification:** Mel spectrograms can be used to identify speakers based on their unique vocal characteristics reflected in the spectrogram patterns.\n"
      ],
      "metadata": {
        "id": "7_Ha0_vnzCCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = librosa.feature.melspectrogram(y=sig, sr=rate, power=2.0, n_mels=512)"
      ],
      "metadata": {
        "id": "sp4fnTxTyZqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the amplitute to decibels\n",
        "logS = librosa.amplitude_to_db(abs(S))"
      ],
      "metadata": {
        "id": "YUgyNi7tzNlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "librosa.display.specshow(logS, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')"
      ],
      "metadata": {
        "id": "M-UPuAdR0KQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "Sz8PKJoQ04uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's listen to the audio\n",
        "Audio(\"/content/sample-12s.mp3\")"
      ],
      "metadata": {
        "id": "xKmv-0fG1Cli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CQT Analysis\n",
        "\n",
        "CQT, which stands for Constant-Q Transform, is another technique used for audio signal analysis. Similar to mel spectrograms, it provides a visual representation of the frequency content of an audio signal over time. However, CQT takes a different approach compared to spectrograms and mel spectrograms.\n",
        "\n",
        "Here's a breakdown of CQT analysis:\n",
        "\n",
        "**1. Underlying Principle:**\n",
        "\n",
        "* A spectrogram uses a fixed window size for analysis across all frequencies. This can be less suitable for capturing the behavior of both high and low frequencies effectively.\n",
        "* CQT overcomes this limitation by using a **constant-Q bandwidth**. This means the frequency resolution changes based on the center frequency of the analysis window. It uses narrower windows for higher frequencies (where rapid changes occur) and wider windows for lower frequencies (where changes are more gradual).\n",
        "\n",
        "**2. Implementation:**\n",
        "\n",
        "* CQT employs a series of overlapping filters with varying bandwidths. These filters are designed to capture a constant range on the mel scale (similar to mel spectrograms) or another psychoacoustic scale.\n",
        "* The audio signal is passed through these filters, and the output represents the signal's strength within each frequency band at a specific point in time.\n",
        "\n",
        "**3. CQT Spectrogram Visualization:**\n",
        "\n",
        "* The resulting data is displayed as a CQT spectrogram. Similar to a mel spectrogram, the x-axis represents time, and the y-axis represents frequency. However, unlike a regular spectrogram where the frequency bins are of equal width, the frequency bins in a CQT spectrogram have varying widths based on the constant-Q principle. Color intensity again indicates the strength of the signal within each time-frequency bin.\n",
        "\n",
        "**Advantages of CQT Analysis:**\n",
        "\n",
        "* **Improved resolution:** CQT provides better resolution for both high and low frequencies compared to traditional spectrograms. This can be beneficial for tasks where capturing rapid changes in high frequencies and subtle variations in low frequencies is crucial.\n",
        "* **Reduced artifacts:** CQT can minimize artifacts like spectral leakage that can occur in traditional spectrograms due to the use of fixed window sizes.\n",
        "\n",
        "**Applications of CQT Analysis:**\n",
        "\n",
        "* **Music analysis:** CQT is useful for analyzing the musical content of audio signals, such as identifying musical notes, chords, and harmonic progressions. The improved resolution at both high and low frequencies can be advantageous for capturing the complex frequency interactions in music.\n",
        "* **Sound event detection:** CQT can be used to identify specific sound events within an audio recording. The time-frequency patterns in the CQT spectrogram can be informative for classifying different types of sounds.\n",
        "* **Audio segmentation:** CQT can be used to segment audio signals into meaningful parts based on the changes in the frequency content over time. This can be helpful for tasks like separating speech from background noise or identifying transitions between different sections in a musical piece."
      ],
      "metadata": {
        "id": "lPBTntKL1n8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = librosa.cqt(sig, sr=rate, n_bins=72)\n",
        "logC = librosa.amplitude_to_db(abs(C))"
      ],
      "metadata": {
        "id": "eYlzvOQl1lTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "librosa.display.specshow(logC, sr=rate, x_axis='time', y_axis='cqt_note', cmap='coolwarm')\n",
        "plt.colorbar(format='%+2.0f dB')"
      ],
      "metadata": {
        "id": "g7WwBRVw1-Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptoeomAl2Bgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}